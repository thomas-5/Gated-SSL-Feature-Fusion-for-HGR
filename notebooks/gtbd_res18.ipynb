{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c1d5e1",
   "metadata": {},
   "source": [
    "Dataset building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2235009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Build train: 100%|██████████| 1600/1600 [00:38<00:00, 41.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: saved crops = 1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Build val: 100%|██████████| 400/400 [00:08<00:00, 44.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: saved crops = 400\n",
      "ImageFolder root: D:\\Courses\\Csc2503\\proj\\ouhands_cls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import cv2, os\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "# === 路径（按你的实际修改）===\n",
    "ROOT = Path(r\"D:\\Courses\\Csc2503\\proj\\archive\\OUHANDS_train\")\n",
    "TRAIN_LIST = ROOT / r\"data_split_for_intermediate_tests\\training_files.txt\"\n",
    "VAL_LIST   = ROOT / r\"data_split_for_intermediate_tests\\validation_files.txt\"\n",
    "COLOUR_DIR = ROOT / r\"train\\hand_data\\colour\"\n",
    "BBOX_DIR   = ROOT / r\"train\\hand_data\\bounding_box\"\n",
    "\n",
    "# 输出的分类数据集（ImageFolder）\n",
    "CLS_ROOT = Path(r\"D:\\Courses\\Csc2503\\proj\\ouhands_cls\")\n",
    "(CLS_ROOT / \"train\").mkdir(parents=True, exist_ok=True)\n",
    "(CLS_ROOT / \"val\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CLASSES = ['A','B','C','D','E','F','H','I','J','K']\n",
    "CLASS2ID = {c:i for i,c in enumerate(CLASSES)}\n",
    "IMG_EXTS = {\".png\",\".jpg\",\".jpeg\",\".bmp\",\".tif\",\".tiff\"}\n",
    "\n",
    "def read_list(fp: Path):\n",
    "    return [ln.strip() for ln in fp.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines() if ln.strip()]\n",
    "\n",
    "def find_img(colour_dir: Path, name: str):\n",
    "    p = colour_dir / name\n",
    "    if p.exists(): return p\n",
    "    stem = Path(name).stem\n",
    "    for e in IMG_EXTS:\n",
    "        q = colour_dir / f\"{stem}{e}\"\n",
    "        if q.exists(): return q\n",
    "    return None\n",
    "\n",
    "def read_boxes_ouhands(txt_path: Path):\n",
    "    \"\"\"\n",
    "    OUHands bbox 行格式（首行是数量N）：\n",
    "      x, w, h, y, [score]\n",
    "    其中：\n",
    "      xmin = x\n",
    "      ymin = w\n",
    "      xmax = x + h\n",
    "      ymax = y + w\n",
    "    返回 [(xmin,ymin,xmax,ymax), ...]\n",
    "    \"\"\"\n",
    "    if not txt_path.exists():\n",
    "        return []\n",
    "    lines = [ln.strip() for ln in txt_path.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines() if ln.strip()]\n",
    "    if len(lines) <= 1: return []\n",
    "    try:\n",
    "        n = int(float(lines[0].split()[0])); rows = lines[1:1+n]\n",
    "    except Exception:\n",
    "        rows = lines\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        parts = [p for p in r.replace(\"\\t\",\" \").split() if p]\n",
    "        if len(parts) < 4: continue\n",
    "        x, w, h, y = map(float, parts[:4])\n",
    "        xmin, ymin = x, w\n",
    "        xmax, ymax = x + h, y + w\n",
    "        if xmax > xmin and ymax > ymin:\n",
    "            out.append((xmin, ymin, xmax, ymax))\n",
    "    return out\n",
    "\n",
    "def crop_and_save(img_path: Path, boxes, out_dir: Path, cls_letter: str, margin=0.15, make_square=True, size=224):\n",
    "    im = cv2.imread(str(img_path))\n",
    "    if im is None: return 0\n",
    "    H, W = im.shape[:2]\n",
    "    saved = 0\n",
    "    # 若多手：按面积从大到小裁（也可全部保留）\n",
    "    boxes = sorted(boxes, key=lambda b:(b[2]-b[0])*(b[3]-b[1]), reverse=True)\n",
    "    for i,(x1,y1,x2,y2) in enumerate(boxes):\n",
    "        # 加一点上下文边框\n",
    "        bw = x2 - x1; bh = y2 - y1\n",
    "        x1m = max(0, int(x1 - margin*bw))\n",
    "        y1m = max(0, int(y1 - margin*bh))\n",
    "        x2m = min(W-1, int(x2 + margin*bw))\n",
    "        y2m = min(H-1, int(y2 + margin*bh))\n",
    "\n",
    "        # 可选：裁成近似正方形（对 ResNet 更友好）\n",
    "        if make_square:\n",
    "            ww = x2m - x1m; hh = y2m - y1m\n",
    "            if ww > hh:\n",
    "                pad = (ww - hh)//2\n",
    "                y1m = max(0, y1m - pad); y2m = min(H-1, y2m + pad)\n",
    "            else:\n",
    "                pad = (hh - ww)//2\n",
    "                x1m = max(0, x1m - pad); x2m = min(W-1, x2m + pad)\n",
    "\n",
    "        crop = im[y1m:y2m+1, x1m:x2m+1]\n",
    "        if crop.size == 0: continue\n",
    "        crop = cv2.resize(crop, (size,size), interpolation=cv2.INTER_AREA)\n",
    "        # 目标类目录\n",
    "        out_cls_dir = out_dir / cls_letter\n",
    "        out_cls_dir.mkdir(parents=True, exist_ok=True)\n",
    "        out_name = f\"{img_path.stem}_{i}.jpg\"\n",
    "        cv2.imwrite(str(out_cls_dir/out_name), crop)\n",
    "        saved += 1\n",
    "    return saved\n",
    "\n",
    "def build_split(split_name: str, list_file: Path):\n",
    "    names = read_list(list_file)\n",
    "    out_dir = CLS_ROOT / split_name\n",
    "    kept = 0\n",
    "    for name in tqdm(names, desc=f\"Build {split_name}\"):\n",
    "        imgp = find_img(COLOUR_DIR, name)\n",
    "        if imgp is None: continue\n",
    "        cls_letter = imgp.name[0].upper()\n",
    "        if cls_letter not in CLASS2ID: \n",
    "            continue\n",
    "        boxes = read_boxes_ouhands(BBOX_DIR / f\"{imgp.stem}.txt\")\n",
    "        if not boxes:\n",
    "            # 没有框：也可选择整体缩放作为负例，或跳过\n",
    "            continue\n",
    "        kept += crop_and_save(imgp, boxes, out_dir, cls_letter, margin=0.15, make_square=True, size=224)\n",
    "    print(f\"{split_name}: saved crops = {kept}\")\n",
    "\n",
    "# 生成 train/val 的裁剪分类集\n",
    "build_split(\"train\", TRAIN_LIST)\n",
    "build_split(\"val\",   VAL_LIST)\n",
    "print(\"ImageFolder root:\", CLS_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8a2111",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3ac44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Epoch 01: train_loss=0.4672  val_loss=0.4203  val_acc=86.25%\n",
      "✅ saved: resnet18_ouhands_best.pt\n",
      "Epoch 02: train_loss=0.0836  val_loss=0.2271  val_acc=93.25%\n",
      "✅ saved: resnet18_ouhands_best.pt\n",
      "Epoch 03: train_loss=0.0316  val_loss=0.3112  val_acc=87.50%\n",
      "Epoch 04: train_loss=0.0326  val_loss=0.1836  val_acc=93.25%\n",
      "Epoch 05: train_loss=0.0241  val_loss=0.1069  val_acc=97.25%\n",
      "✅ saved: resnet18_ouhands_best.pt\n",
      "Epoch 06: train_loss=0.0145  val_loss=0.1011  val_acc=96.50%\n",
      "Epoch 07: train_loss=0.0070  val_loss=0.1042  val_acc=96.00%\n",
      "Epoch 08: train_loss=0.0025  val_loss=0.1108  val_acc=95.75%\n",
      "Epoch 09: train_loss=0.0020  val_loss=0.1013  val_acc=95.75%\n",
      "Epoch 10: train_loss=0.0018  val_loss=0.1010  val_acc=95.75%\n",
      "Epoch 11: train_loss=0.0018  val_loss=0.1098  val_acc=95.75%\n",
      "Epoch 12: train_loss=0.0016  val_loss=0.1043  val_acc=95.50%\n",
      "Epoch 13: train_loss=0.0016  val_loss=0.0947  val_acc=96.25%\n",
      "Epoch 14: train_loss=0.0013  val_loss=0.0814  val_acc=96.25%\n",
      "Epoch 15: train_loss=0.0015  val_loss=0.0872  val_acc=96.75%\n",
      "Epoch 16: train_loss=0.0008  val_loss=0.0697  val_acc=97.25%\n",
      "Epoch 17: train_loss=0.0007  val_loss=0.0455  val_acc=98.75%\n",
      "✅ saved: resnet18_ouhands_best.pt\n",
      "Epoch 18: train_loss=0.0006  val_loss=0.0621  val_acc=97.00%\n",
      "Epoch 19: train_loss=0.0006  val_loss=0.0497  val_acc=98.25%\n",
      "Epoch 20: train_loss=0.0385  val_loss=1.9130  val_acc=60.75%\n",
      "Epoch 21: train_loss=0.1920  val_loss=2.0593  val_acc=55.00%\n",
      "Epoch 22: train_loss=0.1932  val_loss=0.2814  val_acc=88.75%\n",
      "Epoch 23: train_loss=0.0776  val_loss=0.1258  val_acc=95.50%\n",
      "Epoch 24: train_loss=0.0266  val_loss=0.1064  val_acc=96.00%\n",
      "Epoch 25: train_loss=0.0168  val_loss=0.0680  val_acc=97.50%\n",
      "Epoch 26: train_loss=0.0069  val_loss=0.0904  val_acc=97.00%\n",
      "Epoch 27: train_loss=0.0040  val_loss=0.0597  val_acc=97.25%\n",
      "Epoch 28: train_loss=0.0038  val_loss=0.0699  val_acc=97.25%\n",
      "Epoch 29: train_loss=0.0019  val_loss=0.0665  val_acc=97.25%\n",
      "Epoch 30: train_loss=0.0020  val_loss=0.0627  val_acc=97.25%\n",
      "Epoch 31: train_loss=0.0020  val_loss=0.0645  val_acc=97.25%\n",
      "Epoch 32: train_loss=0.0027  val_loss=0.0602  val_acc=97.25%\n",
      "Epoch 33: train_loss=0.0022  val_loss=0.0634  val_acc=97.25%\n",
      "Epoch 34: train_loss=0.0020  val_loss=0.0644  val_acc=97.50%\n",
      "Epoch 35: train_loss=0.0019  val_loss=0.0663  val_acc=97.25%\n",
      "Epoch 36: train_loss=0.0017  val_loss=0.0593  val_acc=97.50%\n",
      "Epoch 37: train_loss=0.0017  val_loss=0.0403  val_acc=98.50%\n",
      "Epoch 38: train_loss=0.0015  val_loss=0.0716  val_acc=97.00%\n",
      "Epoch 39: train_loss=0.0010  val_loss=0.0286  val_acc=98.75%\n",
      "Epoch 40: train_loss=0.0011  val_loss=0.0478  val_acc=98.00%\n",
      "Epoch 41: train_loss=0.0004  val_loss=0.0900  val_acc=97.25%\n",
      "Epoch 42: train_loss=0.0005  val_loss=0.1028  val_acc=97.00%\n",
      "Epoch 43: train_loss=0.0005  val_loss=0.1088  val_acc=97.00%\n",
      "Epoch 44: train_loss=0.0004  val_loss=0.0965  val_acc=96.75%\n",
      "Epoch 45: train_loss=0.0004  val_loss=0.0836  val_acc=97.00%\n",
      "Epoch 46: train_loss=0.0002  val_loss=0.0789  val_acc=97.00%\n",
      "Epoch 47: train_loss=0.0004  val_loss=0.0804  val_acc=96.75%\n",
      "Epoch 48: train_loss=0.0004  val_loss=0.0781  val_acc=96.75%\n",
      "Epoch 49: train_loss=0.0008  val_loss=0.0804  val_acc=96.75%\n",
      "Epoch 50: train_loss=0.0003  val_loss=0.0767  val_acc=96.75%\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from collections import Counter\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 与 ImageNet 预训练一致的归一化\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225]),\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "train_ds = datasets.ImageFolder(root=str(CLS_ROOT/\"train\"), transform=train_tf)\n",
    "val_ds   = datasets.ImageFolder(root=str(CLS_ROOT/\"val\"),   transform=val_tf)\n",
    "\n",
    "# 类不平衡的话加权\n",
    "cnt = Counter([y for _,y in train_ds.samples])\n",
    "num_classes = len(train_ds.classes)\n",
    "class_weights = torch.tensor([len(train_ds)/cnt[i] for i in range(num_classes)], dtype=torch.float32).to(device)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=0, pin_memory=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=128, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "# ResNet18（可换 resnet50）\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)  # 使用类权重\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "            pred = logits.argmax(1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return loss_sum/total, correct/total\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running += loss.item() * x.size(0)\n",
    "\n",
    "    tr_loss = running / len(train_loader.dataset)\n",
    "    va_loss, va_acc = evaluate(model, val_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d}: train_loss={tr_loss:.4f}  val_loss={va_loss:.4f}  val_acc={va_acc*100:.2f}%\")\n",
    "\n",
    "    # 保存最好权重\n",
    "    if va_acc > best_acc:\n",
    "        best_acc = va_acc\n",
    "        torch.save(model.state_dict(), \"resnet18_ouhands_best.pt\")\n",
    "        print(\"✅ saved: resnet18_ouhands_best.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55439bd",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89e5b9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Build cls test: 100%|██████████| 1000/1000 [00:10<00:00, 96.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved crops: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === 原始 OUHands 测试集路径 ===\n",
    "TEST_COLOUR = Path(r\"D:\\Courses\\Csc2503\\proj\\archive\\OUHANDS_test\\test\\hand_data\\colour\")\n",
    "TEST_BBOX   = Path(r\"D:\\Courses\\Csc2503\\proj\\archive\\OUHANDS_test\\test\\hand_data\\bounding_box\")\n",
    "\n",
    "# === 输出的分类测试集（ImageFolder）===\n",
    "CLS_TEST_ROOT = Path(r\"D:\\Courses\\Csc2503\\proj\\ouhands_cls\\test\")\n",
    "CLS_TEST_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CLASSES = ['A','B','C','D','E','F','H','I','J','K']\n",
    "IMG_EXTS = {\".png\",\".jpg\",\".jpeg\",\".bmp\",\".tif\",\".tiff\"}\n",
    "\n",
    "def read_boxes_ouhands(txt_path: Path):\n",
    "    if not txt_path.exists(): return []\n",
    "    lines = [ln.strip() for ln in txt_path.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines() if ln.strip()]\n",
    "    if len(lines) <= 1: return []\n",
    "    try:\n",
    "        n = int(float(lines[0].split()[0])); rows = lines[1:1+n]\n",
    "    except Exception:\n",
    "        rows = lines\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        ps = [p for p in r.replace(\"\\t\",\" \").split() if p]\n",
    "        if len(ps) < 4: continue\n",
    "        x,w,h,y = map(float, ps[:4])\n",
    "        xmin, ymin = x, w\n",
    "        xmax, ymax = x + h, y + w\n",
    "        if xmax > xmin and ymax > ymin:\n",
    "            out.append((xmin, ymin, xmax, ymax))\n",
    "    return out\n",
    "\n",
    "def crop_and_save(img_path: Path, boxes, out_dir: Path, cls_letter: str, margin=0.15, size=224, square=True):\n",
    "    im = cv2.imread(str(img_path))\n",
    "    if im is None: return 0\n",
    "    H,W = im.shape[:2]\n",
    "    saved = 0\n",
    "    # 多框都保留（也可改成只保留最大框）\n",
    "    for i,(x1,y1,x2,y2) in enumerate(boxes):\n",
    "        bw, bh = x2-x1, y2-y1\n",
    "        x1m = max(0, int(x1 - margin*bw))\n",
    "        y1m = max(0, int(y1 - margin*bh))\n",
    "        x2m = min(W-1, int(x2 + margin*bw))\n",
    "        y2m = min(H-1, int(y2 + margin*bh))\n",
    "        if square:\n",
    "            ww, hh = x2m-x1m, y2m-y1m\n",
    "            if ww > hh:\n",
    "                pad = (ww-hh)//2\n",
    "                y1m = max(0, y1m-pad); y2m = min(H-1, y2m+pad)\n",
    "            else:\n",
    "                pad = (hh-ww)//2\n",
    "                x1m = max(0, x1m-pad); x2m = min(W-1, x2m+pad)\n",
    "        crop = im[y1m:y2m+1, x1m:x2m+1]\n",
    "        if crop.size == 0: continue\n",
    "        crop = cv2.resize(crop, (size,size), interpolation=cv2.INTER_AREA)\n",
    "        (out_dir/cls_letter).mkdir(parents=True, exist_ok=True)\n",
    "        cv2.imwrite(str((out_dir/cls_letter/f\"{img_path.stem}_{i}.jpg\")), crop)\n",
    "        saved += 1\n",
    "    return saved\n",
    "\n",
    "count = 0\n",
    "for p in tqdm(sorted(TEST_COLOUR.iterdir()), desc=\"Build cls test\"):\n",
    "    if p.suffix.lower() not in IMG_EXTS: continue\n",
    "    cls_letter = p.name[0].upper()\n",
    "    if cls_letter not in CLASSES: continue\n",
    "    boxes = read_boxes_ouhands(TEST_BBOX / f\"{p.stem}.txt\")\n",
    "    count += crop_and_save(p, boxes, CLS_TEST_ROOT, cls_letter)\n",
    "print(\"Saved crops:\", count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24bd74ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\24912\\AppData\\Local\\Temp\\ipykernel_21100\\3780135154.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd = torch.load(RESNET_WEIGHTS, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1: 0.9000\n",
      "Macro-F1: 0.8992\n",
      "Params (M): 11.182\n",
      "FLOPs (G): 1.82\n",
      "| ResNet18 | CNN (raw image) | 0.9000 | 0.8992 | 11.18 | 1.82 |\n"
     ]
    }
   ],
   "source": [
    "# 如果还没装：!pip -q install scikit-learn thop timm\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from thop import profile\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# === 你的权重路径（你训练时保存的 .pt）===\n",
    "RESNET_WEIGHTS = r\"D:\\Courses\\Csc2503\\proj\\resnet18_ouhands_best.pt\"  # 改成你的文件完整路径\n",
    "\n",
    "# === 分类测试集（ImageFolder 结构）===\n",
    "CLS_TEST_ROOT = Path(r\"D:\\Courses\\Csc2503\\proj\\ouhands_cls\\test\")\n",
    "\n",
    "# === 设备 ===\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# === Data ===\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225]),\n",
    "])\n",
    "test_ds = datasets.ImageFolder(root=str(CLS_TEST_ROOT), transform=test_tf)\n",
    "test_loader = DataLoader(test_ds, batch_size=128, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "# === Model ===\n",
    "num_classes = len(test_ds.classes)  # 应为10\n",
    "model = models.resnet18(weights=None)  # 结构一致\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "sd = torch.load(RESNET_WEIGHTS, map_location=\"cpu\")\n",
    "# 兼容 DataParallel 或纯 state_dict\n",
    "state_dict = sd.get(\"state_dict\", sd)\n",
    "# 去掉 DataParallel 的 'module.' 前缀\n",
    "state_dict = {k.replace(\"module.\", \"\"): v for k,v in state_dict.items()}\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# === 推理并计算 Top-1 与 Macro-F1 ===\n",
    "all_pred, all_true = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(1).cpu().numpy().tolist()\n",
    "        all_pred += pred\n",
    "        all_true += y.numpy().tolist()\n",
    "\n",
    "top1 = accuracy_score(all_true, all_pred)\n",
    "macro_f1 = f1_score(all_true, all_pred, average=\"macro\")\n",
    "\n",
    "print(f\"Top-1: {top1:.4f}\")\n",
    "print(f\"Macro-F1: {macro_f1:.4f}\")\n",
    "\n",
    "# === Params (M) 与 FLOPs (G) ===\n",
    "params_m = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "dummy = torch.randn(1, 3, 224, 224).to(device)\n",
    "flops, params = profile(model, inputs=(dummy,), verbose=False)\n",
    "flops_g = flops / 1e9\n",
    "\n",
    "print(f\"Params (M): {params_m:.3f}\")\n",
    "print(f\"FLOPs (G): {flops_g:.2f}\")\n",
    "\n",
    "# === 一行表格输出 ===\n",
    "print(f\"| ResNet18 | CNN (raw image) | {top1:.4f} | {macro_f1:.4f} | {params_m:.2f} | {flops_g:.2f} |\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2503",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

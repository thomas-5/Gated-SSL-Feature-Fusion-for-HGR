{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c5132e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 1. Import Libraries and Setup Device\n",
    "import os\n",
    "# Fix OpenMP conflict error\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "import sys\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import random\n",
    "\n",
    "# Add current directory to path to import loader\n",
    "sys.path.append(os.getcwd())\n",
    "from ouhands_loader import OuhandsDS\n",
    "\n",
    "# Setup Device\n",
    "device = (\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "    else torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a5e2904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1600 samples for train split\n",
      "Class distribution: {'A': 160, 'B': 160, 'C': 160, 'D': 160, 'E': 160, 'F': 160, 'H': 160, 'I': 160, 'J': 160, 'K': 160}\n",
      "Loaded 400 samples for validation split\n",
      "Class distribution: {'A': 40, 'B': 40, 'C': 40, 'D': 40, 'E': 40, 'F': 40, 'H': 40, 'I': 40, 'J': 40, 'K': 40}\n",
      "Loaded 1000 samples for test split\n",
      "Class distribution: {'A': 100, 'B': 100, 'C': 100, 'D': 100, 'E': 100, 'F': 100, 'H': 100, 'I': 100, 'J': 100, 'K': 100}\n",
      "Train size: 1600\n",
      "Loaded 1000 samples for test split\n",
      "Class distribution: {'A': 100, 'B': 100, 'C': 100, 'D': 100, 'E': 100, 'F': 100, 'H': 100, 'I': 100, 'J': 100, 'K': 100}\n",
      "Train size: 1600\n"
     ]
    }
   ],
   "source": [
    "# 2. Define Barlow Twins Augmentations\n",
    "\n",
    "class BarlowTwinsTransform:\n",
    "    \"\"\"\n",
    "    Generates two augmented views of the same image.\n",
    "    Includes: RandomResizedCrop, HorizontalFlip, ColorJitter, Grayscale, Solarization, GaussianBlur.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=224):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size, scale=(0.4, 1.0)), # Less aggressive crop for small dataset\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n",
    "                p=0.8\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.RandomApply([transforms.GaussianBlur(kernel_size=23)], p=0.1), # Blur\n",
    "            transforms.RandomSolarize(threshold=128, p=0.0), # Disable solarize for hand gestures (might destroy shape)\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Simple transform for validation/testing (no augmentation)\n",
    "        self.test_transform = transforms.Compose([\n",
    "            transforms.Resize((size, size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Return two augmented views\n",
    "        return self.transform(x), self.transform(x)\n",
    "\n",
    "# 3. Dataset Wrapper\n",
    "class BarlowTwinsOuhandsDS(Dataset):\n",
    "    def __init__(self, split='train', root_dir=r'D:\\Courses\\Csc2503\\proj\\archive'):\n",
    "        # Initialize base dataset\n",
    "        # We disable default transform in base_ds to get PIL images\n",
    "        self.base_ds = OuhandsDS(\n",
    "            root_dir=root_dir,\n",
    "            split=split,\n",
    "            transform=lambda x: x \n",
    "        )\n",
    "        self.augmentor = BarlowTwinsTransform(size=224)\n",
    "        self.split = split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get raw PIL image and label\n",
    "        img, label = self.base_ds[idx]\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            # Training: Return two augmented views + label\n",
    "            view1, view2 = self.augmentor(img)\n",
    "            return view1, view2, label\n",
    "        else:\n",
    "            # Validation/Test: Return one standard view + label\n",
    "            # We just use the test_transform defined in augmentor\n",
    "            view = self.augmentor.test_transform(img)\n",
    "            return view, view, label # Return duplicate view to keep signature consistent\n",
    "\n",
    "# Create Datasets\n",
    "train_ds = BarlowTwinsOuhandsDS(split='train')\n",
    "val_ds = BarlowTwinsOuhandsDS(split='validation')\n",
    "test_ds = BarlowTwinsOuhandsDS(split='test')\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "num_workers = 0 # Windows compatibility\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "print(f\"Train size: {len(train_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "410be4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define Barlow Twins Loss\n",
    "\n",
    "class BarlowTwinsLoss(nn.Module):\n",
    "    def __init__(self, lambda_param=0.005, vector_dim=2048):\n",
    "        super(BarlowTwinsLoss, self).__init__()\n",
    "        self.lambda_param = lambda_param\n",
    "        self.vector_dim = vector_dim\n",
    "        self.bn = nn.BatchNorm1d(vector_dim, affine=False) # BN without learnable params\n",
    "\n",
    "    def forward(self, z1, z2):\n",
    "        # z1, z2: (Batch, vector_dim)\n",
    "        \n",
    "        # Empirical cross-correlation matrix\n",
    "        c = self.bn(z1).T @ self.bn(z2)\n",
    "        \n",
    "        # Sum the cross-correlation matrix between all gpus (if distributed)\n",
    "        # Here we are single GPU, so just normalize by batch size\n",
    "        c.div_(z1.size(0))\n",
    "\n",
    "        # Loss\n",
    "        on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()\n",
    "        off_diag = off_diagonal(c).pow_(2).sum()\n",
    "        loss = on_diag + self.lambda_param * off_diag\n",
    "        return loss\n",
    "\n",
    "def off_diagonal(x):\n",
    "    # return a flattened view of the off-diagonal elements of a square matrix\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "036e8f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DINO ViT-S/16 backbone...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\24912/.cache\\torch\\hub\\facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "# 5. Define Hybrid Model (DINO + Barlow Twins)\n",
    "\n",
    "class DINOBarlowTwins(nn.Module):\n",
    "    def __init__(self, num_classes=10, projector_dim=2048):\n",
    "        super(DINOBarlowTwins, self).__init__()\n",
    "        \n",
    "        # Backbone: DINO ViT-S/16\n",
    "        print(\"Loading DINO ViT-S/16 backbone...\")\n",
    "        self.backbone = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
    "        embed_dim = 384\n",
    "        \n",
    "        # Classifier Head (Supervised)\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        # Projector Head (Barlow Twins)\n",
    "        # 3-layer MLP: Linear -> BN -> ReLU -> Linear -> BN -> ReLU -> Linear\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(embed_dim, projector_dim),\n",
    "            nn.BatchNorm1d(projector_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(projector_dim, projector_dim),\n",
    "            nn.BatchNorm1d(projector_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(projector_dim, projector_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Backbone features\n",
    "        features = self.backbone(x) # (B, 384)\n",
    "        \n",
    "        # Classification logits\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        # Projections for Barlow Twins\n",
    "        projections = self.projector(features)\n",
    "        \n",
    "        return logits, projections\n",
    "\n",
    "model = DINOBarlowTwins(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60a683e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with Alpha=0.1 (BT Weight)...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:52<00:00,  1.05s/it]\n",
      "Training: 100%|██████████| 50/50 [00:52<00:00,  1.05s/it]\n",
      "Evaluating: 100%|██████████| 13/13 [00:06<00:00,  1.91it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 117.7415 (CE: 2.5591, BT: 1151.8237) | Acc: 23.50%\n",
      "Val Acc: 33.25%\n",
      "Saved Best Model!\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:42<00:00,  1.17it/s]\n",
      "Training: 100%|██████████| 50/50 [00:42<00:00,  1.17it/s]\n",
      "Evaluating: 100%|██████████| 13/13 [00:04<00:00,  2.73it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 95.0413 (CE: 1.0860, BT: 939.5533) | Acc: 60.62%\n",
      "Val Acc: 58.25%\n",
      "Saved Best Model!\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:42<00:00,  1.17it/s]\n",
      "Training: 100%|██████████| 50/50 [00:42<00:00,  1.17it/s]\n",
      "Evaluating: 100%|██████████| 13/13 [00:04<00:00,  2.69it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 88.2861 (CE: 0.6765, BT: 876.0967) | Acc: 74.38%\n",
      "Val Acc: 63.75%\n",
      "Saved Best Model!\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:42<00:00,  1.17it/s]\n",
      "Training: 100%|██████████| 50/50 [00:42<00:00,  1.17it/s]\n",
      "Evaluating: 100%|██████████| 13/13 [00:04<00:00,  2.65it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 84.3170 (CE: 0.4550, BT: 838.6198) | Acc: 83.94%\n",
      "Val Acc: 70.00%\n",
      "Saved Best Model!\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:43<00:00,  1.14it/s]\n",
      "Training: 100%|██████████| 50/50 [00:43<00:00,  1.14it/s]\n",
      "Evaluating: 100%|██████████| 13/13 [00:05<00:00,  2.57it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 81.7091 (CE: 0.3636, BT: 813.4554) | Acc: 87.69%\n",
      "Val Acc: 79.00%\n",
      "Saved Best Model!\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:42<00:00,  1.17it/s]\n",
      "Training: 100%|██████████| 50/50 [00:42<00:00,  1.17it/s]\n",
      "Evaluating: 100%|██████████| 13/13 [00:04<00:00,  2.70it/s]\n",
      "Evaluating: 100%|██████████| 13/13 [00:04<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 80.6224 (CE: 0.2821, BT: 803.4031) | Acc: 90.94%\n",
      "Val Acc: 69.75%\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:43<00:00,  1.16it/s]\n",
      "Training: 100%|██████████| 50/50 [00:43<00:00,  1.16it/s]\n",
      "Evaluating: 100%|██████████| 13/13 [00:04<00:00,  2.68it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 80.0966 (CE: 0.2177, BT: 798.7896) | Acc: 93.31%\n",
      "Val Acc: 81.25%\n",
      "Saved Best Model!\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:43<00:00,  1.15it/s]\n",
      "Training: 100%|██████████| 50/50 [00:43<00:00,  1.15it/s]\n",
      "Evaluating: 100%|██████████| 13/13 [00:05<00:00,  2.59it/s]\n",
      "Evaluating: 100%|██████████| 13/13 [00:05<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 78.5920 (CE: 0.1663, BT: 784.2570) | Acc: 96.00%\n",
      "Val Acc: 75.75%\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:43<00:00,  1.16it/s]\n",
      "Training: 100%|██████████| 50/50 [00:43<00:00,  1.16it/s]\n",
      "Evaluating: 100%|██████████| 13/13 [00:04<00:00,  2.62it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 77.8319 (CE: 0.1393, BT: 776.9261) | Acc: 96.94%\n",
      "Val Acc: 82.00%\n",
      "Saved Best Model!\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:43<00:00,  1.16it/s]\n",
      "Training: 100%|██████████| 50/50 [00:43<00:00,  1.16it/s]\n",
      "Evaluating: 100%|██████████| 13/13 [00:04<00:00,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 77.0165 (CE: 0.1331, BT: 768.8340) | Acc: 96.75%\n",
      "Val Acc: 81.50%\n",
      "Training Complete. Best Val Acc: 82.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Training Loop (Hybrid Loss)\n",
    "\n",
    "# Losses\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "criterion_bt = BarlowTwinsLoss(lambda_param=0.005, vector_dim=2048).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# Weight for Barlow Twins Loss\n",
    "alpha = 0.1 # Balance between CE and BT loss\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_ce = 0.0\n",
    "    running_bt = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for view1, view2, labels in tqdm(loader, desc=\"Training\"):\n",
    "        view1, view2, labels = view1.to(device), view2.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass for both views\n",
    "        logits1, proj1 = model(view1)\n",
    "        logits2, proj2 = model(view2)\n",
    "        \n",
    "        # 1. Supervised Loss (CrossEntropy)\n",
    "        # We can use logits from both views or just one. Let's use both for robustness.\n",
    "        loss_ce = (criterion_ce(logits1, labels) + criterion_ce(logits2, labels)) / 2\n",
    "        \n",
    "        # 2. Self-Supervised Loss (Barlow Twins)\n",
    "        loss_bt = criterion_bt(proj1, proj2)\n",
    "        \n",
    "        # Total Loss\n",
    "        loss = loss_ce + alpha * loss_bt\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        running_ce += loss_ce.item() * labels.size(0)\n",
    "        running_bt += loss_bt.item() * labels.size(0)\n",
    "        \n",
    "        # Accuracy (using view1)\n",
    "        _, predicted = logits1.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, running_ce/total, running_bt/total, epoch_acc\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for view1, _, labels in tqdm(loader, desc=\"Evaluating\"):\n",
    "            view1, labels = view1.to(device), labels.to(device)\n",
    "            logits, _ = model(view1)\n",
    "            _, predicted = logits.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "    return 100. * correct / total\n",
    "\n",
    "# Run Training\n",
    "num_epochs = 10\n",
    "best_acc = 0.0\n",
    "\n",
    "print(f\"Starting training with Alpha={alpha} (BT Weight)...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    loss, loss_ce, loss_bt, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    val_acc = evaluate(model, val_loader, device)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Train Loss: {loss:.4f} (CE: {loss_ce:.4f}, BT: {loss_bt:.4f}) | Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_dino_bt_model.pth\")\n",
    "        print(\"Saved Best Model!\")\n",
    "\n",
    "print(f\"Training Complete. Best Val Acc: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51cfb744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\24912\\AppData\\Local\\Temp\\ipykernel_14108\\833585563.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_dino_bt_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:19<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "FINAL RESULTS (DINO + Barlow Twins)\n",
      "==============================\n",
      "Metric          | Value     \n",
      "------------------------------\n",
      "Top-1 Acc (%)   | 65.80\n",
      "Macro-F1        | 0.6552\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Final Evaluation\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "if os.path.exists(\"best_dino_bt_model.pth\"):\n",
    "    model.load_state_dict(torch.load(\"best_dino_bt_model.pth\"))\n",
    "    print(\"Loaded best model.\")\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for view1, _, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        view1 = view1.to(device)\n",
    "        logits, _ = model(view1)\n",
    "        _, preds = logits.max(1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "test_acc = np.mean(np.array(all_preds) == np.array(all_labels)) * 100\n",
    "test_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"FINAL RESULTS (DINO + Barlow Twins)\")\n",
    "print(\"=\"*30)\n",
    "print(f\"{'Metric':<15} | {'Value':<10}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"{'Top-1 Acc (%)':<15} | {test_acc:.2f}\")\n",
    "print(f\"{'Macro-F1':<15} | {test_f1:.4f}\")\n",
    "print(\"=\"*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2503",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

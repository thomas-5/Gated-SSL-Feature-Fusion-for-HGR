{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f47b5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1225ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# === 训练/验证用的 OUHands 路径（原始整图）===\n",
    "ROOT = Path(r\"D:\\Courses\\Csc2503\\proj\\archive\\OUHANDS_train\")\n",
    "TRAIN_LIST = ROOT / r\"data_split_for_intermediate_tests\\training_files.txt\"\n",
    "VAL_LIST   = ROOT / r\"data_split_for_intermediate_tests\\validation_files.txt\"\n",
    "COLOUR_DIR = ROOT / r\"train\\hand_data\\colour\"\n",
    "\n",
    "# === 测试集（原始整图）===\n",
    "TEST_COLOUR = Path(r\"D:\\Courses\\Csc2503\\proj\\archive\\OUHANDS_test\\test\\hand_data\\colour\")\n",
    "\n",
    "# 类别（A..K 跳过 G）\n",
    "CLASSES = ['A','B','C','D','E','F','H','I','J','K']\n",
    "CLASS2ID = {c:i for i,c in enumerate(CLASSES)}\n",
    "IMG_EXTS = {\".png\",\".jpg\",\".jpeg\",\".bmp\",\".tif\",\".tiff\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfd601c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def read_list(fp: Path):\n",
    "    return [ln.strip() for ln in fp.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines() if ln.strip()]\n",
    "\n",
    "def resolve_image_path(colour_dir: Path, name: str):\n",
    "    p = colour_dir / name\n",
    "    if p.exists(): return p\n",
    "    stem = Path(name).stem\n",
    "    for e in IMG_EXTS:\n",
    "        q = colour_dir / f\"{stem}{e}\"\n",
    "        if q.exists(): return q\n",
    "    return None\n",
    "\n",
    "class RawImageListDataset(Dataset):\n",
    "    \"\"\"\n",
    "    从文件名首字母得到类别，不使用 bbox；整图 -> ResNet 输入\n",
    "    \"\"\"\n",
    "    def __init__(self, colour_dir: Path, names: list[str], transform=None):\n",
    "        self.items = []\n",
    "        self.transform = transform\n",
    "        for nm in names:\n",
    "            p = resolve_image_path(colour_dir, nm)\n",
    "            if p is None: \n",
    "                continue\n",
    "            letter = p.name[0].upper()\n",
    "            if letter not in CLASS2ID:\n",
    "                continue\n",
    "            self.items.append((p, CLASS2ID[letter]))\n",
    "        if len(self.items) == 0:\n",
    "            raise RuntimeError(\"No images collected. Check paths and filelists.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    def __getitem__(self, idx):\n",
    "        p, y = self.items[idx]\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, y\n",
    "\n",
    "# transforms（ImageNet 预处理风格）\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(0.1,0.1,0.1,0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225]),\n",
    "])\n",
    "eval_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# 列表 -> 数据集\n",
    "train_names = read_list(TRAIN_LIST)\n",
    "val_names   = read_list(VAL_LIST)\n",
    "# 测试集：直接遍历文件夹\n",
    "test_names  = [p.name for p in TEST_COLOUR.iterdir() if p.suffix.lower() in IMG_EXTS]\n",
    "\n",
    "train_ds = RawImageListDataset(COLOUR_DIR, train_names, transform=train_tf)\n",
    "val_ds   = RawImageListDataset(COLOUR_DIR, val_names,   transform=eval_tf)\n",
    "test_ds  = RawImageListDataset(TEST_COLOUR, test_names, transform=eval_tf)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,  num_workers=0, pin_memory=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=128, shuffle=False, num_workers=0, pin_memory=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=128, shuffle=False, num_workers=0, pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "816f5e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/50 | train_loss=0.6439  val_loss=0.9097  val_top1=69.50%  val_macroF1=0.6856\n",
      "✅ saved: resnet18_raw_best.pt\n",
      "Epoch 02/50 | train_loss=0.1416  val_loss=0.3047  val_top1=89.50%  val_macroF1=0.8963\n",
      "✅ saved: resnet18_raw_best.pt\n",
      "Epoch 03/50 | train_loss=0.0927  val_loss=0.9233  val_top1=78.00%  val_macroF1=0.7624\n",
      "Epoch 04/50 | train_loss=0.0984  val_loss=1.6225  val_top1=68.50%  val_macroF1=0.6910\n",
      "Epoch 05/50 | train_loss=0.0689  val_loss=0.7233  val_top1=77.00%  val_macroF1=0.7744\n",
      "Epoch 06/50 | train_loss=0.0403  val_loss=0.3600  val_top1=89.25%  val_macroF1=0.8924\n",
      "Epoch 07/50 | train_loss=0.0239  val_loss=0.4249  val_top1=86.25%  val_macroF1=0.8642\n",
      "Epoch 08/50 | train_loss=0.0127  val_loss=0.2184  val_top1=93.25%  val_macroF1=0.9319\n",
      "✅ saved: resnet18_raw_best.pt\n",
      "Epoch 09/50 | train_loss=0.0093  val_loss=0.3086  val_top1=89.50%  val_macroF1=0.8957\n",
      "Epoch 10/50 | train_loss=0.0043  val_loss=0.2050  val_top1=92.50%  val_macroF1=0.9235\n",
      "Epoch 11/50 | train_loss=0.0036  val_loss=0.2567  val_top1=90.25%  val_macroF1=0.9023\n",
      "Epoch 12/50 | train_loss=0.0018  val_loss=0.2407  val_top1=90.50%  val_macroF1=0.9048\n",
      "Epoch 13/50 | train_loss=0.0040  val_loss=0.2410  val_top1=90.50%  val_macroF1=0.9049\n",
      "Epoch 14/50 | train_loss=0.0015  val_loss=0.2418  val_top1=90.50%  val_macroF1=0.9051\n",
      "Epoch 15/50 | train_loss=0.0032  val_loss=0.2322  val_top1=90.75%  val_macroF1=0.9082\n",
      "Epoch 16/50 | train_loss=0.0024  val_loss=0.2423  val_top1=90.50%  val_macroF1=0.9049\n",
      "Epoch 17/50 | train_loss=0.0022  val_loss=0.2347  val_top1=90.25%  val_macroF1=0.9029\n",
      "Epoch 18/50 | train_loss=0.0016  val_loss=0.2457  val_top1=90.25%  val_macroF1=0.9024\n",
      "Epoch 19/50 | train_loss=0.0023  val_loss=0.2229  val_top1=90.50%  val_macroF1=0.9060\n",
      "Epoch 20/50 | train_loss=0.0013  val_loss=0.2288  val_top1=90.75%  val_macroF1=0.9085\n",
      "Epoch 21/50 | train_loss=0.0010  val_loss=0.2369  val_top1=90.75%  val_macroF1=0.9086\n",
      "Epoch 22/50 | train_loss=0.0019  val_loss=0.2623  val_top1=89.75%  val_macroF1=0.8975\n",
      "Epoch 23/50 | train_loss=0.0012  val_loss=0.2668  val_top1=90.25%  val_macroF1=0.9026\n",
      "Epoch 24/50 | train_loss=0.0008  val_loss=0.2663  val_top1=90.50%  val_macroF1=0.9059\n",
      "Epoch 25/50 | train_loss=0.0015  val_loss=0.2397  val_top1=91.25%  val_macroF1=0.9130\n",
      "Epoch 26/50 | train_loss=0.0017  val_loss=0.3075  val_top1=89.75%  val_macroF1=0.8983\n",
      "Epoch 27/50 | train_loss=0.0435  val_loss=0.8300  val_top1=78.25%  val_macroF1=0.7650\n",
      "Epoch 28/50 | train_loss=0.2142  val_loss=2.9625  val_top1=51.00%  val_macroF1=0.4744\n",
      "Epoch 29/50 | train_loss=0.2297  val_loss=1.0071  val_top1=72.75%  val_macroF1=0.7425\n",
      "Epoch 30/50 | train_loss=0.1106  val_loss=1.3173  val_top1=74.25%  val_macroF1=0.7398\n",
      "Epoch 31/50 | train_loss=0.0655  val_loss=0.5014  val_top1=84.25%  val_macroF1=0.8449\n",
      "Epoch 32/50 | train_loss=0.0920  val_loss=1.6791  val_top1=64.00%  val_macroF1=0.6422\n",
      "Epoch 33/50 | train_loss=0.1076  val_loss=0.5664  val_top1=82.25%  val_macroF1=0.8174\n",
      "Epoch 34/50 | train_loss=0.0616  val_loss=0.4988  val_top1=83.00%  val_macroF1=0.8310\n",
      "Epoch 35/50 | train_loss=0.0134  val_loss=0.6001  val_top1=81.50%  val_macroF1=0.8067\n",
      "Epoch 36/50 | train_loss=0.0091  val_loss=0.3524  val_top1=89.75%  val_macroF1=0.9001\n",
      "Epoch 37/50 | train_loss=0.0049  val_loss=0.3254  val_top1=89.75%  val_macroF1=0.9003\n",
      "Epoch 38/50 | train_loss=0.0028  val_loss=0.3044  val_top1=89.25%  val_macroF1=0.8938\n",
      "Epoch 39/50 | train_loss=0.0016  val_loss=0.3085  val_top1=89.75%  val_macroF1=0.8990\n",
      "Epoch 40/50 | train_loss=0.0019  val_loss=0.3135  val_top1=90.25%  val_macroF1=0.9040\n",
      "Epoch 41/50 | train_loss=0.0028  val_loss=0.2673  val_top1=92.25%  val_macroF1=0.9233\n",
      "Epoch 42/50 | train_loss=0.0035  val_loss=0.2680  val_top1=92.00%  val_macroF1=0.9209\n",
      "Epoch 43/50 | train_loss=0.0009  val_loss=0.2750  val_top1=92.25%  val_macroF1=0.9236\n",
      "Epoch 44/50 | train_loss=0.0020  val_loss=0.2713  val_top1=91.75%  val_macroF1=0.9188\n",
      "Epoch 45/50 | train_loss=0.0011  val_loss=0.2653  val_top1=92.25%  val_macroF1=0.9236\n",
      "Epoch 46/50 | train_loss=0.0033  val_loss=0.2665  val_top1=92.25%  val_macroF1=0.9233\n",
      "Epoch 47/50 | train_loss=0.0010  val_loss=0.2660  val_top1=92.00%  val_macroF1=0.9209\n",
      "Epoch 48/50 | train_loss=0.0013  val_loss=0.2672  val_top1=92.25%  val_macroF1=0.9233\n",
      "Epoch 49/50 | train_loss=0.0013  val_loss=0.2634  val_top1=92.25%  val_macroF1=0.9234\n",
      "Epoch 50/50 | train_loss=0.0025  val_loss=0.2686  val_top1=92.25%  val_macroF1=0.9234\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# 类不平衡权重\n",
    "cnt = Counter([y for _,y in train_ds.items])\n",
    "class_weights = torch.tensor([len(train_ds)/cnt[i] for i in range(len(CLASSES))], dtype=torch.float32).to(device)\n",
    "\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(model.fc.in_features, len(CLASSES))\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    losses, preds, trues = 0.0, [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            losses += loss.item() * x.size(0)\n",
    "            preds.extend(logits.argmax(1).cpu().tolist())\n",
    "            trues.extend(y.cpu().tolist())\n",
    "    avg_loss = losses / len(loader.dataset)\n",
    "    top1 = accuracy_score(trues, preds)\n",
    "    macro_f1 = f1_score(trues, preds, average=\"macro\")\n",
    "    return avg_loss, top1, macro_f1\n",
    "\n",
    "best_val = 0.0\n",
    "EPOCHS = 50\n",
    "for ep in range(EPOCHS):\n",
    "    model.train()\n",
    "    run = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        run += loss.item() * x.size(0)\n",
    "    train_loss = run / len(train_loader.dataset)\n",
    "    val_loss, val_top1, val_f1 = evaluate(model, val_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {ep+1:02d}/{EPOCHS} | train_loss={train_loss:.4f}  \"\n",
    "          f\"val_loss={val_loss:.4f}  val_top1={val_top1*100:.2f}%  val_macroF1={val_f1:.4f}\")\n",
    "\n",
    "    if val_top1 > best_val:\n",
    "        best_val = val_top1\n",
    "        torch.save(model.state_dict(), \"resnet18_raw_best.pt\")\n",
    "        print(\"✅ saved: resnet18_raw_best.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e45b6b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\24912\\AppData\\Local\\Temp\\ipykernel_23768\\567573906.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd = torch.load(\"resnet18_raw_best.pt\", map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Test ==\n",
      "Top-1: 0.7470\n",
      "Macro-F1: 0.7466\n"
     ]
    }
   ],
   "source": [
    "# 载入最佳权重（如已保存）\n",
    "sd = torch.load(\"resnet18_raw_best.pt\", map_location=\"cpu\")\n",
    "model.load_state_dict(sd, strict=False)\n",
    "\n",
    "test_loss, test_top1, test_f1 = evaluate(model, test_loader)\n",
    "print(f\"\\n== Test ==\")\n",
    "print(f\"Top-1: {test_top1:.4f}\")\n",
    "print(f\"Macro-F1: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "851564eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params (M): 11.182\n",
      "FLOPs  (G): 1.82\n",
      "\n",
      "| ResNet18 (raw image) | None | 0.7470 | 0.7466 | 11.18 | 1.82 |\n"
     ]
    }
   ],
   "source": [
    "!pip -q install thop\n",
    "\n",
    "from thop import profile\n",
    "params_m = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "dummy = torch.randn(1, 3, 224, 224).to(device)\n",
    "flops, _ = profile(model, inputs=(dummy,), verbose=False)\n",
    "flops_g = flops / 1e9\n",
    "print(f\"Params (M): {params_m:.3f}\")\n",
    "print(f\"FLOPs  (G): {flops_g:.2f}\")\n",
    "\n",
    "print(f\"\\n| ResNet18 (raw image) | None | {test_top1:.4f} | {test_f1:.4f} | {params_m:.2f} | {flops_g:.2f} |\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2503",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
